{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\multilevelmontecarlolearning\\new-multi-simple.ipynb Cell 1\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/new-multi-simple.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/new-multi-simple.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/new-multi-simple.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/new-multi-simple.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/new-multi-simple.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m init_ops\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_probability'"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "#Multilevel algorithm using 8 networks\n",
    "#For more detailed explanations of the training and model parameters\n",
    "#see Gerstner et al. \"Multilevel Monte Carlo learning.\" arXiv preprint arXiv:2102.08734 (2021).\n",
    "\n",
    "#Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
    "#The framework was modified in such a way that it generates networks for each of the level estimators\n",
    "\n",
    "#Packages\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "\n",
    "def neural_net(x, xi_approx, neurons, is_training, name, net_id, mv_decay=0.9, dtype=tf.float32):\n",
    "    def approx_test(): return xi_approx\n",
    "    def approx_learn(): return x\n",
    "    x = tf.cond(is_training, approx_learn, approx_test)\n",
    "\n",
    "    def _batch_normalization(_x):\n",
    "        beta = tf.get_variable(f'beta{net_id}', [_x.get_shape()[-1]], dtype, init_ops.zeros_initializer())\n",
    "        gamma = tf.get_variable(f'gamma{net_id}', [_x.get_shape()[-1]], dtype, init_ops.ones_initializer())\n",
    "        mv_mean = tf.get_variable(f'mv_mean{net_id}', [_x.get_shape()[-1]], dtype, init_ops.zeros_initializer(), trainable=False)\n",
    "        mv_variance = tf.get_variable(f'mv_variance{net_id}', [_x.get_shape()[-1]], dtype, init_ops.ones_initializer(), trainable=False)\n",
    "        mean, variance = tf.nn.moments(_x, [0], name=f'moments{net_id}')\n",
    "        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, assign_moving_average(mv_mean, mean, mv_decay, True))\n",
    "        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, assign_moving_average(mv_variance, variance, mv_decay, False))\n",
    "        mean, variance = tf.cond(is_training, lambda: (mean, variance), lambda: (mv_mean, mv_variance))\n",
    "        return tf.nn.batch_normalization(_x, mean, variance, beta, gamma, 1e-6)\n",
    "\n",
    "    def _layer(_x, out_size, activation_fn):\n",
    "        w = tf.get_variable(f'weights{net_id}', [_x.get_shape().as_list()[-1], out_size], dtype, tf.initializers.glorot_uniform())\n",
    "        return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        x = _batch_normalization(x)\n",
    "        for i in range(len(neurons)):\n",
    "            with tf.variable_scope(f'layer{net_id}_{i + 1}_'):\n",
    "                x = _layer(x, neurons[i], tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
    "    return x\n",
    "\n",
    "def train_and_test(xi_list, phi_list, xi_approx, u_reference_GBM, u_reference_list, neurons, train_steps,mc_rounds, mc_freq, file_name, dtype=tf.float32):\n",
    "  \n",
    "  def _approximate_errors():\n",
    "      gs_lr = sess.run([global_step[i] for i in range(amountNetworks)] + [learning_rate[i] for i in range(amountNetworks)])\n",
    "      gs = gs_lr[:amountNetworks]\n",
    "      lr = gs_lr[amountNetworks:]\n",
    "      li_err = [0. for _ in range(amountNetworks)]\n",
    "      li_err_kombination = 0.\n",
    "      for _ in range(mc_rounds):\n",
    "          li = sess.run([err_l_inf[i] for i in range(amountNetworks)], feed_dict={is_training: False})\n",
    "          appr_ref_kombination = sess.run([approx[i] for i in range(amountNetworks)] + [reference, err_l_kombination], feed_dict={is_training: False})\n",
    "          appr = appr_ref_kombination[:amountNetworks]\n",
    "          ref = appr_ref_kombination[amountNetworks]\n",
    "          li_kombination = appr_ref_kombination[-1]\n",
    "          for i in range(amountNetworks):\n",
    "              li_err[i] = np.maximum(li_err[i], li[i])\n",
    "          li_err_kombination = np.maximum(li_err_kombination, li_kombination)\n",
    "      t_mc = time.time()\n",
    "      file_out.write(f'{gs[0]}, {li_err_kombination}, {lr[0]}, {t1_train - t0_train}, {t_mc - t1_train}\\n')\n",
    "      file_out.flush()\n",
    "  \n",
    "  t0_train = time.time()\n",
    "  is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "  amountNetworks = len(phi_list)\n",
    "  u_approx = []\n",
    "  for i in range(amountNetworks):\n",
    "      u_approx.append(neural_net(xi_list[i], xi_approx, neurons, is_training, f'u_approx_{i}', str(i), dtype=dtype))\n",
    "\n",
    "  loss = [tf.reduce_mean(tf.squared_difference(u_approx[i], phi_list[i])) for i in range(amountNetworks)]\n",
    "\n",
    "  approx = [tf.reduce_mean(u_approx[i]) for i in range(amountNetworks)]\n",
    "  reference = tf.reduce_mean(u_reference_GBM)\n",
    "\n",
    "  err = [tf.abs(u_approx[i] - u_reference_list[i]) for i in range(amountNetworks)]\n",
    "\n",
    "  err_kombination = tf.abs(sum(u_approx) - u_reference_GBM)\n",
    "\n",
    "  err_l_inf = [tf.reduce_max(err[i]) for i in range(len(err))]\n",
    "  err_l_kombination = tf.reduce_max(err_kombination)\n",
    "\n",
    "  lr = [0.01, 0.01]\n",
    "  step_rate = [40000, 40000]\n",
    "  decay = [0.1, 0.1]\n",
    "  global_step = [tf.Variable(1, trainable=False), tf.Variable(0, trainable=False)]\n",
    "  increment_global_step = [tf.assign(global_step[i], global_step[i] + 1) for i in range(amountNetworks)]\n",
    "  learning_rate = [tf.train.exponential_decay(lr[i], global_step[i], step_rate[i], decay[i], staircase=True) for i in range(amountNetworks)]\n",
    "  optimizer = [tf.train.AdamOptimizer(learning_rate[i]) for i in range(amountNetworks)]\n",
    "  update_ops = [tf.get_collection(tf.GraphKeys.UPDATE_OPS, f'u_approx_{i}') for i in range(amountNetworks)]\n",
    "  \n",
    "  train_op = []\n",
    "  for i in range(len(update_ops)):\n",
    "    with tf.control_dependencies(update_ops[i]):\n",
    "      train_op.append(optimizer[i].minimize(loss[i], global_step[i])) \n",
    "        \n",
    "  file_out = open(file_name, 'w')\n",
    "  file_out.write('step,li_err, learning_rate, time_train, time_mc  \\n ')\n",
    "    \n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1,train_steps):\n",
    "      if step % mc_freq == 0:\n",
    "        print(step)\n",
    "        t1_train = time.time()\n",
    "        _approximate_errors()\n",
    "        t0_train = time.time()      \n",
    "      sess.run(train_op, feed_dict={is_training:True})\n",
    "    t1_train = time.time()\n",
    "    _approximate_errors()\n",
    "  file_out.close()\n",
    "  \n",
    "#Model and training parameter specification  \n",
    "for i in range(1,2):\n",
    " #print(i)\n",
    " tf.reset_default_graph()\n",
    " tf.random.set_random_seed(i)\n",
    " with tf.Session()  as sess:\n",
    "  dtype = tf.float32\n",
    "  #Set network and training parameter (same number of training steps for each network)\n",
    "  batch_sizes = [7500, 1817]\n",
    "  batch_size_approx=2000\n",
    "  d = 5\n",
    "  # Level adaptation parameter: steps = M ^ l\n",
    "  M = 2 # only 2 is supported\n",
    "  maximumLevel = 1 # P_0 + P_1-P_0, here 1 is the maximumLevel\n",
    "  amountSteps = [M**i for i in range(maximumLevel+1)]\n",
    "\n",
    "  neurons = [50, 50, 1]\n",
    "  train_steps = 150000\n",
    " \n",
    "  Ksteps_p1_p0=train_steps\n",
    "  Ksteps_p0=   train_steps    \n",
    "  mc_rounds, mc_freq = 100, 10\n",
    "\n",
    "  mc_samples_ref, mc_rounds_ref_p0, mc_rounds_ref_p1_p0 = 1, 1000000,1000000\n",
    "\n",
    "  # Define the intervals for the parameters\n",
    "  s_0_l = 80.0\n",
    "  s_0_r = 120.0\n",
    "  sigma_l = 0.1\n",
    "  sigma_r = 0.2\n",
    "  mu_l = 0.02\n",
    "  mu_r = 0.05\n",
    "  T_l = 0.9\n",
    "  T_r = 1.0\n",
    "  K_l = 109.0\n",
    "  K_r = 110.0\n",
    "\n",
    "  # Define the modifiers for the approximation intervals\n",
    "  s_0_modifier = 0.4\n",
    "  sigma_modifier = 0.01\n",
    "  mu_modifier = 0.01\n",
    "  T_modifier = 0.01\n",
    "  K_modifier = 0.1\n",
    "\n",
    "  # Use the modifiers to define the intervals for the approximations of the parameters\n",
    "  s_0_l_approx = s_0_l + s_0_modifier\n",
    "  s_0_r_approx = s_0_r - s_0_modifier\n",
    "  sigma_l_approx = sigma_l + sigma_modifier\n",
    "  sigma_r_approx = sigma_r - sigma_modifier\n",
    "  mu_l_approx = mu_l + mu_modifier\n",
    "  mu_r_approx = mu_r - mu_modifier\n",
    "  T_l_approx = T_l + T_modifier\n",
    "  T_r_approx = T_r - T_modifier\n",
    "  K_l_approx = K_l + K_modifier\n",
    "  K_r_approx = K_r - K_modifier\n",
    "\n",
    "  # Training intervals\n",
    "  s0 = tf.random_uniform((batch_sizes[0],1), minval=s_0_l,\n",
    "                                 maxval=s_0_r, dtype=dtype)\n",
    "  sigma=tf.random_uniform((batch_sizes[0],1),\n",
    "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
    "  mu=tf.random_uniform((batch_sizes[0],1),\n",
    "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
    "  T=tf.random_uniform((batch_sizes[0],1),\n",
    "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
    "  K=tf.random_uniform((batch_sizes[0],1),\n",
    "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
    "  s0_p1_p0 = tf.stack((tf.random_uniform((batch_sizes[1],1), minval=s_0_l,\n",
    "                                 maxval=s_0_r, dtype=dtype)))\n",
    "  sigma_p1_p0=tf.random_uniform((batch_sizes[1],1),\n",
    "                            minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
    "  mu_p1_p0=tf.random_uniform((batch_sizes[1],1),\n",
    "                            minval=mu_l,maxval=mu_r, dtype=dtype)\n",
    "  T_p1_p0=tf.random_uniform((batch_sizes[1],1),\n",
    "                            minval=T_l,maxval=T_r, dtype=dtype)\n",
    "  K_p1_p0=tf.random_uniform((batch_sizes[1],1),\n",
    "                      minval=K_l,maxval=K_r, dtype=dtype)\n",
    "  xi=tf.reshape(tf.stack([s0,sigma,mu,T,K], axis=2), (batch_sizes[0],d))\n",
    "  xi_p1_p0=tf.reshape(tf.stack([s0_p1_p0,sigma_p1_p0,mu_p1_p0,T_p1_p0,K_p1_p0], axis=2), (batch_sizes[1],d))\n",
    "  xi_list = [xi, xi_p1_p0]\n",
    "\n",
    "  # Approximation intervals\n",
    "  s0_approx = tf.random_uniform((batch_size_approx,  1 ), \n",
    "                            minval=s_0_l_approx,maxval=s_0_r_approx, dtype=dtype)\n",
    "  sigma_approx=tf.random_uniform((batch_size_approx,  1 ), \n",
    "                            minval=sigma_l_approx,maxval=sigma_r_approx, dtype=dtype)\n",
    "  mu_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                            minval=mu_l_approx,maxval=mu_r_approx, dtype=dtype)\n",
    "  T_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                            minval=T_l_approx,maxval=T_r_approx, dtype=dtype)\n",
    "  K_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                            minval=K_l_approx,maxval=K_r_approx, dtype=dtype)\n",
    "  \n",
    "  xi_approx=tf.reshape(tf.stack([s0_approx,sigma_approx,mu_approx,T_approx,K_approx], axis=2), (batch_size_approx,d))\n",
    "\n",
    "  # References: Black-Scholes formula as reference\n",
    "  tfd = tfp.distributions\n",
    "  dist = tfd.Normal(loc=tf.cast(0.,tf.float32), scale=tf.cast(1.,tf.float32))\n",
    "  d1=tf.math.divide(\n",
    "  (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx + 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
    "  d2=tf.math.divide(\n",
    "  (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx - 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
    "\n",
    "  u_reference= tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))\n",
    "\n",
    " def phi(x,sigma,mu,T,K, axis=1):\n",
    "    payoff=tf.exp(-mu * T)* tf.maximum(x - K, 0.)\n",
    "    return payoff\n",
    "\n",
    " # First level: actually just P_0 without level estimator\n",
    " def batch_sizes_level0(idx, s,sigma,mu,T,K, samples): \n",
    "    z = tf.random_normal(shape=(samples, batch_sizes[0], 1),\n",
    "                         stddev=1., dtype=dtype)\n",
    "    h=T/amountSteps[0]\n",
    "    s=s + mu *s * h +sigma * s *tf.sqrt(h)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h)*z)**2-h)\n",
    "    return tf.add(idx, 1), s, sigma,mu,T,K\n",
    " def mc_body_level0(idx, p):\n",
    "    _, _x, _sigma,_mu,_T,_K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < amountSteps[0],\n",
    "                          lambda _idx, s, sigma,mu,T,K: batch_sizes_level0(_idx, s, sigma,mu,T,K,\n",
    "                                                   mc_samples_ref),\n",
    "                                                   loop_var_mc[0])\n",
    "    return idx + 1, p + tf.reduce_mean(phi(_x,_sigma,_mu,_T,_K, 2), axis=0)  \n",
    "    \n",
    " #Multilevel Monte Carlo level estimators\n",
    " def sde_body_levelEstimator(idx, s, sfine, sigma, mu, T, K, samples, amountSteps): \n",
    "    z1 = tf.random_normal(shape=(samples, batch_sizes[1], 1),\n",
    "                          stddev=1., dtype=dtype)\n",
    "    z2 = tf.random_normal(shape=(samples, batch_sizes[1], 1),\n",
    "                          stddev=1., dtype=dtype)\n",
    "    z=(z1+z2)/tf.sqrt(2.)\n",
    "    hcoarse= T / amountSteps\n",
    "    hfine= T / (amountSteps * 2)\n",
    "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
    "    sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
    "    s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
    "    return tf.add(idx, 1), s, sfine, sigma, mu, T, K\n",
    "\n",
    " def mc_body_levelEstimator(idx, p, amountSteps):\n",
    "    _, _xcoarse, _xfine, sigma, mu, T, K = tf.while_loop(lambda _idx, s, xfine, sigma, mu, T, K: _idx < amountSteps,\n",
    "                                                          lambda _idx, s, xfine, sigma, mu, T, K: sde_body_levelEstimator(_idx, s, xfine, sigma, mu, T, K, mc_samples_ref, amountSteps),\n",
    "                                                          loop_var_mc[1])\n",
    "    return idx + 1, p + tf.reduce_mean(phi(_xfine, sigma, mu, T, K, 2) - phi(_xcoarse, sigma, mu, T, K, 2), axis=0)\n",
    "\n",
    " loop_var_mc = []\n",
    " loop_var_mc.append((tf.constant(0),tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * s0, tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * sigma,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * mu,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * T,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * K))\n",
    " loop_var_mc.append((tf.constant(0),tf.ones((mc_samples_ref,batch_sizes[1], 1), dtype) * s0_p1_p0,tf.ones((mc_samples_ref,batch_sizes[1], 1), dtype) * s0_p1_p0, tf.ones((mc_samples_ref,batch_sizes[1], 1), dtype) * sigma_p1_p0,tf.ones((mc_samples_ref,batch_sizes[1], 1), dtype) * mu_p1_p0,tf.ones((mc_samples_ref,batch_sizes[1], 1), dtype) * T_p1_p0,tf.ones((mc_samples_ref,batch_sizes[1], 1), dtype) * K_p1_p0))\n",
    " \n",
    " _, x_sde,sigma,mu,T,K = tf.while_loop(lambda idx, s, sigma,mu,T,K: idx < amountSteps[0],\n",
    "                         lambda idx, s, sigma,mu,T,K: batch_sizes_level0(idx, s,sigma,mu,T,K, 1),\n",
    "                         loop_var_mc[0])\n",
    " u_list = []\n",
    " u_list.append(tf.while_loop(lambda idx, p: idx < 1, mc_body_level0,(tf.constant(0), tf.zeros((batch_sizes[0], 1), dtype)))[1])\n",
    " level = 1\n",
    " u_list.append(tf.while_loop(lambda idx, p: idx < 1, lambda idx, p: mc_body_levelEstimator(idx, p, amountSteps[level-1]), (tf.constant(0), tf.zeros((batch_sizes[level], 1), dtype)))[1])\n",
    "\n",
    " phi_list = []\n",
    " phi_list.append(u_list[0] / tf.cast(1, tf.float32))\n",
    " phi_list.append(u_list[1] / tf.cast(1, tf.float32))\n",
    " \n",
    " u_reference_list =[]\n",
    " u_reference_list.append(tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2)))\n",
    " u_reference_list.append(xi_approx*0.)\n",
    "\n",
    "\n",
    " #Start training and testing                        \n",
    " train_and_test(xi_list, phi_list, xi_approx, u_reference, u_reference_list,neurons, train_steps,mc_rounds, mc_freq, 'multi-introductury.csv', dtype)                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
