{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/da-roth/NeuronalNetworkTensorflowFramework/blob/main/multilevelmontecarlolearning/new_introductory_example_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU support =  False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input tensor 'while/while/Enter_1:0' enters the loop with shape (1, 75000, 1), but has shape (200, 75000, 1) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\multilevelmontecarlolearning\\refactored_introductory_example_multi.ipynb Cell 2\u001b[0m in \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=357'>358</a>\u001b[0m         \u001b[39m#Start training and testing                        \u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=358'>359</a>\u001b[0m         train_and_test(Regressor, TrainSettings,xi_list, phi_list, xi_approx, u_reference, u_reference_list, neurons, \u001b[39m'\u001b[39m\u001b[39mmulti-introductory-new-2.csv\u001b[39m\u001b[39m'\u001b[39m, dtype)         \n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=360'>361</a>\u001b[0m train_and_test_Multilevel(Generator, Regressor, TrainSettings)\n",
      "\u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\multilevelmontecarlolearning\\refactored_introductory_example_multi.ipynb Cell 2\u001b[0m in \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=344'>345</a>\u001b[0m phi_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=345'>346</a>\u001b[0m u_reference_list \u001b[39m=\u001b[39m[]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=347'>348</a>\u001b[0m u_list\u001b[39m.\u001b[39mappend(tf\u001b[39m.\u001b[39;49mwhile_loop(\u001b[39mlambda\u001b[39;49;00m idx, p: idx \u001b[39m<\u001b[39;49m \u001b[39m1\u001b[39;49m, Generator\u001b[39m.\u001b[39;49mMonteCarlo_loop_level0,(tf\u001b[39m.\u001b[39;49mconstant(\u001b[39m0\u001b[39;49m), tf\u001b[39m.\u001b[39;49mzeros((batch_sizes[\u001b[39m0\u001b[39;49m], \u001b[39m1\u001b[39;49m), dtype)))[\u001b[39m1\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=348'>349</a>\u001b[0m phi_list\u001b[39m.\u001b[39mappend(u_list[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m tf\u001b[39m.\u001b[39mcast(\u001b[39m1\u001b[39m, tf\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=349'>350</a>\u001b[0m u_reference_list\u001b[39m.\u001b[39mappend(tf\u001b[39m.\u001b[39mmultiply(s0_approx,(dist\u001b[39m.\u001b[39mcdf(d1)))\u001b[39m-\u001b[39mK_approx\u001b[39m*\u001b[39mtf\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mmu_approx\u001b[39m*\u001b[39mT_approx)\u001b[39m*\u001b[39m(dist\u001b[39m.\u001b[39mcdf(d2)))\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2792\u001b[0m, in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2790\u001b[0m \u001b[39mif\u001b[39;00m loop_context\u001b[39m.\u001b[39mouter_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2791\u001b[0m   ops\u001b[39m.\u001b[39madd_to_collection(ops\u001b[39m.\u001b[39mGraphKeys\u001b[39m.\u001b[39mWHILE_CONTEXT, loop_context)\n\u001b[1;32m-> 2792\u001b[0m result \u001b[39m=\u001b[39m loop_context\u001b[39m.\u001b[39;49mBuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m   2793\u001b[0m                                 return_same_structure)\n\u001b[0;32m   2794\u001b[0m \u001b[39mif\u001b[39;00m maximum_iterations \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2795\u001b[0m   \u001b[39mreturn\u001b[39;00m result[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2276\u001b[0m, in \u001b[0;36mWhileContext.BuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[0;32m   2272\u001b[0m   \u001b[39m# _BuildLoop calls _update_input in several places. _mutation_lock()\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m   \u001b[39m# ensures a Session.run call cannot occur between creating and mutating\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m   \u001b[39m# new ops.\u001b[39;00m\n\u001b[0;32m   2275\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_mutation_lock():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2276\u001b[0m     original_body_result, exit_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_BuildLoop(\n\u001b[0;32m   2277\u001b[0m         pred, body, flat_orig_loop_vars, flat_loop_vars,\n\u001b[0;32m   2278\u001b[0m         loop_vars_signature)\n\u001b[0;32m   2279\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   2280\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mExit()\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2195\u001b[0m, in \u001b[0;36mWhileContext._BuildLoop\u001b[1;34m(self, pred, body, flat_orig_loop_vars, flat_loop_vars, loop_vars_signature)\u001b[0m\n\u001b[0;32m   2190\u001b[0m packed_vars_for_body \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   2191\u001b[0m     structure\u001b[39m=\u001b[39mloop_vars_signature,\n\u001b[0;32m   2192\u001b[0m     flat_sequence\u001b[39m=\u001b[39mvars_for_body_with_tensorarrays,\n\u001b[0;32m   2193\u001b[0m     expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   2194\u001b[0m pre_summaries \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_collection(ops\u001b[39m.\u001b[39mGraphKeys\u001b[39m.\u001b[39m_SUMMARY_COLLECTION)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2195\u001b[0m body_result \u001b[39m=\u001b[39m body(\u001b[39m*\u001b[39;49mpacked_vars_for_body)\n\u001b[0;32m   2196\u001b[0m post_summaries \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_collection(ops\u001b[39m.\u001b[39mGraphKeys\u001b[39m.\u001b[39m_SUMMARY_COLLECTION)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nest\u001b[39m.\u001b[39mis_nested(body_result):\n",
      "\u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\multilevelmontecarlolearning\\refactored_introductory_example_multi.ipynb Cell 2\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mMonteCarlo_loop_level0\u001b[39m(\u001b[39mself\u001b[39m, idx, p):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     _, _x, _sigma,_mu,_T,_K \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mwhile_loop(\u001b[39mlambda\u001b[39;49;00m _idx, s, sigma,mu,T,K: _idx \u001b[39m<\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stepsPerLevel[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                         \u001b[39mlambda\u001b[39;49;00m _idx, s, sigma,mu,T,K: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMilstein_level0(_idx, s, sigma,mu,T,K,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m                                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mc_samples_ref),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loop_var_mc[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, p \u001b[39m+\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mphi(_x,_sigma,_mu,_T,_K, \u001b[39m2\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2792\u001b[0m, in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2790\u001b[0m \u001b[39mif\u001b[39;00m loop_context\u001b[39m.\u001b[39mouter_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2791\u001b[0m   ops\u001b[39m.\u001b[39madd_to_collection(ops\u001b[39m.\u001b[39mGraphKeys\u001b[39m.\u001b[39mWHILE_CONTEXT, loop_context)\n\u001b[1;32m-> 2792\u001b[0m result \u001b[39m=\u001b[39m loop_context\u001b[39m.\u001b[39;49mBuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m   2793\u001b[0m                                 return_same_structure)\n\u001b[0;32m   2794\u001b[0m \u001b[39mif\u001b[39;00m maximum_iterations \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2795\u001b[0m   \u001b[39mreturn\u001b[39;00m result[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2276\u001b[0m, in \u001b[0;36mWhileContext.BuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[0;32m   2272\u001b[0m   \u001b[39m# _BuildLoop calls _update_input in several places. _mutation_lock()\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m   \u001b[39m# ensures a Session.run call cannot occur between creating and mutating\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m   \u001b[39m# new ops.\u001b[39;00m\n\u001b[0;32m   2275\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_mutation_lock():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2276\u001b[0m     original_body_result, exit_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_BuildLoop(\n\u001b[0;32m   2277\u001b[0m         pred, body, flat_orig_loop_vars, flat_loop_vars,\n\u001b[0;32m   2278\u001b[0m         loop_vars_signature)\n\u001b[0;32m   2279\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   2280\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mExit()\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2238\u001b[0m, in \u001b[0;36mWhileContext._BuildLoop\u001b[1;34m(self, pred, body, flat_orig_loop_vars, flat_loop_vars, loop_vars_signature)\u001b[0m\n\u001b[0;32m   2236\u001b[0m next_vars \u001b[39m=\u001b[39m []\n\u001b[0;32m   2237\u001b[0m \u001b[39mfor\u001b[39;00m m, v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(merge_vars, result):\n\u001b[1;32m-> 2238\u001b[0m   next_vars\u001b[39m.\u001b[39mappend(_AddNextAndBackEdge(m, v))\n\u001b[0;32m   2240\u001b[0m \u001b[39m# Add the exit ops.\u001b[39;00m\n\u001b[0;32m   2241\u001b[0m exit_vars \u001b[39m=\u001b[39m [exit(x[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m switch_vars]\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:559\u001b[0m, in \u001b[0;36m_AddNextAndBackEdge\u001b[1;34m(m, v, enforce_shape_invariant)\u001b[0m\n\u001b[0;32m    553\u001b[0m   v \u001b[39m=\u001b[39m _NextIteration(v)\n\u001b[0;32m    554\u001b[0m   \u001b[39mif\u001b[39;00m enforce_shape_invariant:\n\u001b[0;32m    555\u001b[0m     \u001b[39m# Make sure the shapes of loop outputs are correct. We do this before\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[39m# calling _update_input, which will raise a less-helpful error message if\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     \u001b[39m# the types don't match.\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[39m# TODO(skyewm): call this for other cases below (needs testing)\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     _EnforceShapeInvariant(m, v)\n\u001b[0;32m    560\u001b[0m   m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39m_update_input(\u001b[39m1\u001b[39m, v)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(m, composite_tensor\u001b[39m.\u001b[39mCompositeTensor):\n\u001b[0;32m    562\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:539\u001b[0m, in \u001b[0;36m_EnforceShapeInvariant\u001b[1;34m(merge_var, next_var)\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[39massert\u001b[39;00m util\u001b[39m.\u001b[39mIsLoopEnter(enter)\n\u001b[0;32m    538\u001b[0m     input_t \u001b[39m=\u001b[39m enter\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 539\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    540\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInput tensor \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m enters the loop with shape \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, but has shape \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mafter one iteration. To allow the shape to vary across iterations, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse the `shape_invariants` argument of tf.while_loop to specify a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mless-specific shape.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (input_t\u001b[39m.\u001b[39mname, input_t\u001b[39m.\u001b[39mshape, n_shape))\n\u001b[0;32m    544\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    545\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmerge_var\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a Tensor. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(merge_var)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Input tensor 'while/while/Enter_1:0' enters the loop with shape (1, 75000, 1), but has shape (200, 75000, 1) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape."
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "#Multilevel algorithm using 8 networks\n",
    "#For more detailed explanations of the training and model parameters\n",
    "#see Gerstner et al. \"Multilevel Monte Carlo learning.\" arXiv preprint arXiv:2102.08734 (2021).\n",
    "\n",
    "#Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
    "#The framework was modified in such a way that it generates networks for each of the level estimators\n",
    "\n",
    "import os\n",
    "mainDirectory = os.path.abspath(os.path.join(os.getcwd() ,'..'))\n",
    "packageFile = os.path.abspath(os.path.join(mainDirectory, 'montecarlolearning', 'packages.py'))\n",
    "exec(open(packageFile).read())\n",
    "\n",
    "#Packages\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "\n",
    "class GBM_Multilevel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def set_batch_sizes(self, value):\n",
    "        self._batch_sizes = value\n",
    "\n",
    "    def set_stepsPerLevel(self, value):\n",
    "        self._stepsPerLevel = value\n",
    "\n",
    "    def set_dtype(self, value):\n",
    "        self._dtype = value\n",
    "\n",
    "    def set_mc_samples_ref(self, value):\n",
    "        self._mc_samples_ref = value\n",
    "\n",
    "    def set_loop_var_mc(self, value):\n",
    "        self._loop_var_mc = value\n",
    "\n",
    "    def phi(x,sigma,mu,T,K, axis=1):\n",
    "        payoff=tf.exp(-mu * T)* tf.maximum(x - K, 0.)\n",
    "        return payoff\n",
    "    \n",
    "    # First level: actually just P_0 without level estimator\n",
    "    def Milstein_level0(self, idx, s,sigma,mu,T,K, samples): \n",
    "        z = tf.random_normal(shape=(samples, self._batch_sizes[0], 1),\n",
    "                            stddev=1., dtype=self._dtype)\n",
    "        h=T/self._stepsPerLevel[0]\n",
    "        s=s + mu *s * h +sigma * s *tf.sqrt(h)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h)*z)**2-h)\n",
    "        return tf.add(idx, 1), s, sigma,mu,T,K\n",
    "    \n",
    "    def MonteCarlo_loop_level0(self, idx, p):\n",
    "        _, _x, _sigma,_mu,_T,_K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < self._stepsPerLevel[0],\n",
    "                            lambda _idx, s, sigma,mu,T,K: self.Milstein_level0(_idx, s, sigma,mu,T,K,\n",
    "                                                    self._mc_samples_ref),\n",
    "                                                    self._loop_var_mc[0])\n",
    "        return idx + 1, p + tf.reduce_mean(self.phi(_x,_sigma,_mu,_T,_K, 2), axis=0)  \n",
    "        \n",
    "    #Multilevel Monte Carlo level estimators\n",
    "    def Milstein_levelEstimator(self, idx, s, sfine, sigma, mu, T, K, samples, level): \n",
    "        z1 = tf.random_normal(shape=(samples, self._batch_sizes[level], 1),\n",
    "                            stddev=1., dtype=self._dtype)\n",
    "        z2 = tf.random_normal(shape=(samples, self._batch_sizes[level], 1),\n",
    "                            stddev=1., dtype=self._dtype)\n",
    "        z=(z1+z2)/tf.sqrt(2.)\n",
    "        amountSteps = self._stepsPerLevel[level-1]\n",
    "        hcoarse= T / amountSteps\n",
    "        hfine= T / (amountSteps * 2)\n",
    "        sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
    "        sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
    "        s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
    "        return tf.add(idx, 1), s, sfine, sigma, mu, T, K\n",
    "\n",
    "    def MonteCarlo_loop_levelEstimator(self, idx, p, level):\n",
    "        amountSteps = self._stepsPerLevel[level-1]\n",
    "        _, _xcoarse, _xfine, sigma, mu, T, K = tf.while_loop(lambda _idx, s, xfine, sigma, mu, T, K: _idx < amountSteps,\n",
    "                                                            lambda _idx, s, xfine, sigma, mu, T, K: self.Milstein_levelEstimator(_idx, s, xfine, sigma, mu, T, K, self._mc_samples_ref, level),\n",
    "                                                            self._loop_var_mc[level])\n",
    "        return idx + 1, p + tf.reduce_mean(self.phi(_xfine, sigma, mu, T, K, 2) - self.phi(_xcoarse, sigma, mu, T, K, 2), axis=0)\n",
    "\n",
    "\n",
    "class Neural_Approximator_Multilevel:\n",
    "        # Setter for data Generator\n",
    "    def set_Generator(self, Generator):\n",
    "        self._Generator = Generator\n",
    "        \n",
    "    @property\n",
    "    def Generator(self):\n",
    "        return self._Generator\n",
    "\n",
    "    # Setter for hiddenNeurons\n",
    "    def set_hiddenNeurons(self, hiddenNeurons):\n",
    "        self._hiddenNeurons = hiddenNeurons\n",
    "\n",
    "    # Setter for hiddenLayers\n",
    "    def set_hiddenLayers(self, hiddenLayers):\n",
    "        self._hiddenLayers = hiddenLayers\n",
    "        \n",
    "    @property\n",
    "    def HiddenNeurons(self):\n",
    "        return self._hiddenNeurons\n",
    "    \n",
    "    @property\n",
    "    def HiddenLayers(self):\n",
    "        return self._hiddenLayers\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def neural_net(x, xi_approx, neurons, is_training, name, net_id, mv_decay=0.9, dtype=tf.float32):\n",
    "        def approx_test(): return xi_approx\n",
    "        def approx_learn(): return x\n",
    "        x = tf.cond(is_training, approx_learn, approx_test)\n",
    "\n",
    "        def _batch_normalization(_x):\n",
    "            beta = tf.get_variable(f'beta{net_id}', [_x.get_shape()[-1]], dtype, init_ops.zeros_initializer())\n",
    "            gamma = tf.get_variable(f'gamma{net_id}', [_x.get_shape()[-1]], dtype, init_ops.ones_initializer())\n",
    "            mv_mean = tf.get_variable(f'mv_mean{net_id}', [_x.get_shape()[-1]], dtype, init_ops.zeros_initializer(), trainable=False)\n",
    "            mv_variance = tf.get_variable(f'mv_variance{net_id}', [_x.get_shape()[-1]], dtype, init_ops.ones_initializer(), trainable=False)\n",
    "            mean, variance = tf.nn.moments(_x, [0], name=f'moments{net_id}')\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, assign_moving_average(mv_mean, mean, mv_decay, True))\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, assign_moving_average(mv_variance, variance, mv_decay, False))\n",
    "            mean, variance = tf.cond(is_training, lambda: (mean, variance), lambda: (mv_mean, mv_variance))\n",
    "            return tf.nn.batch_normalization(_x, mean, variance, beta, gamma, 1e-6)\n",
    "\n",
    "        def _layer(_x, out_size, activation_fn):\n",
    "            w = tf.get_variable(f'weights{net_id}', [_x.get_shape().as_list()[-1], out_size], dtype, tf.initializers.glorot_uniform())\n",
    "            return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            x = _batch_normalization(x)\n",
    "            for i in range(len(neurons)):\n",
    "                with tf.variable_scope(f'layer{net_id}_{i + 1}_'):\n",
    "                    x = _layer(x, neurons[i], tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
    "        return x\n",
    "\n",
    "def train_and_test(Regressor, TrainSettings, xi_list, phi_list, xi_approx, u_reference_GBM, u_reference_list, neurons, file_name, dtype=tf.float32):\n",
    "    \n",
    "    def _approximate_errors():\n",
    "        gs_lr = sess.run([global_step[i] for i in range(amountNetworks)] + [learning_rate[i] for i in range(amountNetworks)])\n",
    "        gs = gs_lr[:amountNetworks]\n",
    "        lr = gs_lr[amountNetworks:]\n",
    "        li_err = [0. for _ in range(amountNetworks)]\n",
    "        li_err_kombination = 0.\n",
    "        for _ in range(TrainSettings.mcRounds):\n",
    "            li = sess.run([err_l_inf[i] for i in range(amountNetworks)], feed_dict={is_training: False})\n",
    "            appr_ref_kombination = sess.run([approx[i] for i in range(amountNetworks)] + [reference, err_l_kombination], feed_dict={is_training: False})\n",
    "            appr = appr_ref_kombination[:amountNetworks]\n",
    "            ref = appr_ref_kombination[amountNetworks]\n",
    "            li_kombination = appr_ref_kombination[-1]\n",
    "            for i in range(amountNetworks):\n",
    "                li_err[i] = np.maximum(li_err[i], li[i])\n",
    "            li_err_kombination = np.maximum(li_err_kombination, li_kombination)\n",
    "        t_mc = time.time()\n",
    "        file_out.write(f'{gs[0]}, {li_err_kombination}, {lr[0]}, {t1_train - t0_train}, {t_mc - t1_train}\\n')\n",
    "        file_out.flush()\n",
    "    \n",
    "    t0_train = time.time()\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    amountNetworks = len(phi_list)\n",
    "    u_approx = []\n",
    "    for i in range(amountNetworks):\n",
    "        u_approx.append(Regressor.neural_net(xi_list[i], xi_approx, neurons, is_training, f'u_approx_{i}', str(i), dtype=dtype))\n",
    "\n",
    "    loss = [tf.reduce_mean(tf.squared_difference(u_approx[i], phi_list[i])) for i in range(amountNetworks)]\n",
    "\n",
    "    approx = [tf.reduce_mean(u_approx[i]) for i in range(amountNetworks)]\n",
    "    reference = tf.reduce_mean(u_reference_GBM)\n",
    "\n",
    "    err = [tf.abs(u_approx[i] - u_reference_list[i]) for i in range(amountNetworks)]\n",
    "\n",
    "    err_kombination = tf.abs(sum(u_approx) - u_reference_GBM)\n",
    "\n",
    "    err_l_inf = [tf.reduce_max(err[i]) for i in range(len(err))]\n",
    "    err_l_kombination = tf.reduce_max(err_kombination)\n",
    "\n",
    "    lr = [TrainSettings.learningRateSchedule[0] for _ in range(amountNetworks)]\n",
    "    step_rate = [TrainSettings.learningRateSchedule[2] for _ in range(amountNetworks)]\n",
    "    decay = [TrainSettings.learningRateSchedule[1] for _ in range(amountNetworks)]\n",
    "    global_step = [tf.Variable(1, trainable=False) for _ in range(amountNetworks)]\n",
    "    increment_global_step = [tf.assign(global_step[i], global_step[i] + 1) for i in range(amountNetworks)]\n",
    "    learning_rate = [tf.train.exponential_decay(lr[i], global_step[i], step_rate[i], decay[i], staircase=True) for i in range(amountNetworks)]\n",
    "    optimizer = [tf.train.AdamOptimizer(learning_rate[i]) for i in range(amountNetworks)]\n",
    "    update_ops = [tf.get_collection(tf.GraphKeys.UPDATE_OPS, f'u_approx_{i}') for i in range(amountNetworks)]\n",
    "    \n",
    "    train_op = []\n",
    "    for i in range(len(update_ops)):\n",
    "        with tf.control_dependencies(update_ops[i]):\n",
    "            train_op.append(optimizer[i].minimize(loss[i], global_step[i])) \n",
    "            \n",
    "    file_out = open(file_name, 'w')\n",
    "    file_out.write('step,li_err, learning_rate, time_train, time_mc  \\n ')\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(1,TrainSettings.TrainingSteps):\n",
    "            if step % TrainSettings.testFrequency == 0:\n",
    "                print(step)\n",
    "                t1_train = time.time()\n",
    "                _approximate_errors()\n",
    "                t0_train = time.time()      \n",
    "        sess.run(train_op, feed_dict={is_training:True})\n",
    "        t1_train = time.time()\n",
    "        _approximate_errors()\n",
    "    file_out.close()\n",
    "\n",
    "Generator = GBM_Multilevel()\n",
    "  \n",
    "Regressor = Neural_Approximator_Multilevel()\n",
    "Regressor.set_hiddenNeurons(50)\n",
    "Regressor.set_hiddenLayers(2)\n",
    "\n",
    "TrainSettings = TrainingSettings()\n",
    "TrainSettings.set_learning_rate_schedule([0.01, 0.1, 1000])\n",
    "TrainSettings.set_test_frequency(150)\n",
    "TrainSettings.set_mcRounds(100)\n",
    "TrainSettings.set_nTest(200)\n",
    "TrainSettings.set_samplesPerStep([75000, 1817, 690, 264, 93, 33, 12, 5])\n",
    "TrainSettings.set_trainingSteps(150000)\n",
    "\n",
    "def train_and_test_Multilevel(Generator, Regressor, TrainSettings):\n",
    "\n",
    "    Generator.set_batch_sizes(TrainSettings.SamplesPerStep)\n",
    "    \n",
    "    Generator.set_dtype(tf.float32)\n",
    "    Generator.set_mc_samples_ref(TrainSettings.nTest)\n",
    "\n",
    "    #Model and training parameter specification  \n",
    "    for i in range(1,2):\n",
    "        #print(i)\n",
    "        tf.reset_default_graph()\n",
    "        tf.random.set_random_seed(i)\n",
    "        with tf.Session()  as sess:\n",
    "            dtype = tf.float32\n",
    "            #Set network and training parameter (same number of training steps for each network)\n",
    "            batch_sizes = TrainSettings.SamplesPerStep # original [75000, 1817, 690, 264, 93, 33, 12, 5]\n",
    "            batch_size_approx= TrainSettings.nTest# original 2000000\n",
    "            d = 5\n",
    "            # Level adaptation parameter: steps = M ^ l\n",
    "            M = 2\n",
    "            maximumLevel = len(batch_sizes) - 1 # P_0 + P_1-P_0, here 1 is the maximumLevel\n",
    "            stepsPerLevel = [M**i for i in range(maximumLevel)]\n",
    "            Generator.set_stepsPerLevel(stepsPerLevel )\n",
    "\n",
    "            neurons = [Regressor.HiddenNeurons for _ in range(Regressor.HiddenLayers)] + [1]\n",
    "            train_steps = TrainSettings.TrainingSteps # original 150000\n",
    "            \n",
    "            Ksteps_p1_p0=train_steps\n",
    "            Ksteps_p0=   train_steps    \n",
    "            mc_rounds, mc_freq = TrainSettings.mcRounds, TrainSettings.testFrequency # original  100, 10\n",
    "\n",
    "            mc_samples_ref, mc_rounds_ref_p0, mc_rounds_ref_p1_p0 = 1, 1000000,1000000\n",
    "\n",
    "            # Define the intervals for the parameters\n",
    "            s_0_l = 80.0\n",
    "            s_0_r = 120.0\n",
    "            sigma_l = 0.1\n",
    "            sigma_r = 0.2\n",
    "            mu_l = 0.02\n",
    "            mu_r = 0.05\n",
    "            T_l = 0.9\n",
    "            T_r = 1.0\n",
    "            K_l = 109.0\n",
    "            K_r = 110.0\n",
    "\n",
    "            # Define the modifiers for the approximation intervals\n",
    "            s_0_modifier = 0.4\n",
    "            sigma_modifier = 0.01\n",
    "            mu_modifier = 0.01\n",
    "            T_modifier = 0.01\n",
    "            K_modifier = 0.1\n",
    "\n",
    "            # Use the modifiers to define the intervals for the approximations of the parameters\n",
    "            s_0_l_approx = s_0_l + s_0_modifier\n",
    "            s_0_r_approx = s_0_r - s_0_modifier\n",
    "            sigma_l_approx = sigma_l + sigma_modifier\n",
    "            sigma_r_approx = sigma_r - sigma_modifier\n",
    "            mu_l_approx = mu_l + mu_modifier\n",
    "            mu_r_approx = mu_r - mu_modifier\n",
    "            T_l_approx = T_l + T_modifier\n",
    "            T_r_approx = T_r - T_modifier\n",
    "            K_l_approx = K_l + K_modifier\n",
    "            K_r_approx = K_r - K_modifier\n",
    "\n",
    "            # Training intervals\n",
    "            s0 = tf.random_uniform((batch_sizes[0],1), minval=s_0_l,\n",
    "                                            maxval=s_0_r, dtype=dtype)\n",
    "            sigma=tf.random_uniform((batch_sizes[0],1),\n",
    "                                        minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
    "            mu=tf.random_uniform((batch_sizes[0],1),\n",
    "                                        minval=mu_l,maxval=mu_r, dtype=dtype)\n",
    "            T=tf.random_uniform((batch_sizes[0],1),\n",
    "                                        minval=T_l,maxval=T_r, dtype=dtype)\n",
    "            K=tf.random_uniform((batch_sizes[0],1),\n",
    "                                minval=K_l,maxval=K_r, dtype=dtype)\n",
    "            \n",
    "            xi_level0=tf.reshape(tf.stack([s0,sigma,mu,T,K], axis=2), (batch_sizes[0],d))\n",
    "\n",
    "            xi_list = []\n",
    "            xi_list.append(xi_level0)\n",
    "\n",
    "            loop_var_mc = []\n",
    "            loop_var_mc.append((tf.constant(0),tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * s0, tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * sigma,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * mu,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * T,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * K))\n",
    "            \n",
    "            Generator.set_loop_var_mc(loop_var_mc)\n",
    "\n",
    "            for i in range(1,len(batch_sizes)):\n",
    "                s0_level_estimator = tf.stack((tf.random_uniform((batch_sizes[i],1), minval=s_0_l, maxval=s_0_r, dtype=dtype)))\n",
    "                sigma_level_estimator = tf.random_uniform((batch_sizes[i],1), minval=sigma_l, maxval=sigma_r, dtype=dtype)\n",
    "                mu_level_estimator = tf.random_uniform((batch_sizes[i],1), minval=mu_l, maxval=mu_r, dtype=dtype)\n",
    "                T_level_estimator = tf.random_uniform((batch_sizes[i],1), minval=T_l, maxval=T_r, dtype=dtype)\n",
    "                K_level_estimator = tf.random_uniform((batch_sizes[i],1), minval=K_l, maxval=K_r, dtype=dtype)\n",
    "                xi_level_estimator= tf.reshape(tf.stack([s0_level_estimator, sigma_level_estimator, mu_level_estimator, T_level_estimator, K_level_estimator], axis=2), (batch_sizes[i], d))\n",
    "                xi_list.append(xi_level_estimator)\n",
    "                loop_var_mc.append((tf.constant(0),tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * s0_level_estimator,tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * s0_level_estimator, tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * sigma_level_estimator,tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * mu_level_estimator,tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * T_level_estimator,tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * K_level_estimator))\n",
    "\n",
    "            # Approximation intervals\n",
    "            s0_approx = tf.random_uniform((batch_size_approx,  1 ), \n",
    "                                        minval=s_0_l_approx,maxval=s_0_r_approx, dtype=dtype)\n",
    "            sigma_approx=tf.random_uniform((batch_size_approx,  1 ), \n",
    "                                        minval=sigma_l_approx,maxval=sigma_r_approx, dtype=dtype)\n",
    "            mu_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                                        minval=mu_l_approx,maxval=mu_r_approx, dtype=dtype)\n",
    "            T_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                                        minval=T_l_approx,maxval=T_r_approx, dtype=dtype)\n",
    "            K_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                                        minval=K_l_approx,maxval=K_r_approx, dtype=dtype)\n",
    "            xi_approx=tf.reshape(tf.stack([s0_approx,sigma_approx,mu_approx,T_approx,K_approx], axis=2), (batch_size_approx,d))\n",
    "\n",
    "            # References: Black-Scholes formula as reference\n",
    "            tfd = tfp.distributions\n",
    "            dist = tfd.Normal(loc=tf.cast(0.,tf.float32), scale=tf.cast(1.,tf.float32))\n",
    "            d1=tf.math.divide(\n",
    "            (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx + 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
    "            d2=tf.math.divide(\n",
    "            (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx - 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
    "\n",
    "            u_reference= tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))\n",
    "\n",
    "        u_list = []\n",
    "        phi_list = []\n",
    "        u_reference_list =[]\n",
    "\n",
    "        u_list.append(tf.while_loop(lambda idx, p: idx < 1, Generator.MonteCarlo_loop_level0,(tf.constant(0), tf.zeros((batch_sizes[0], 1), dtype)))[1])\n",
    "        phi_list.append(u_list[0] / tf.cast(1, tf.float32))\n",
    "        u_reference_list.append(tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2)))\n",
    "\n",
    "\n",
    "        for i in range(1,len(batch_sizes)):\n",
    "            u_list.append(tf.while_loop(lambda idx, p: idx < 1, lambda idx, p: Generator.MonteCarlo_loop_levelEstimator(idx, p, i), (tf.constant(0), tf.zeros((batch_sizes[i], 1), dtype)))[1])\n",
    "            phi_list.append(u_list[i] / tf.cast(1, tf.float32))\n",
    "            u_reference_list.append(xi_approx*0.)\n",
    "\n",
    "        #Start training and testing                        \n",
    "        train_and_test(Regressor, TrainSettings,xi_list, phi_list, xi_approx, u_reference, u_reference_list, neurons, 'multi-introductory-new-2.csv', dtype)         \n",
    "\n",
    "train_and_test_Multilevel(Generator, Regressor, TrainSettings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
