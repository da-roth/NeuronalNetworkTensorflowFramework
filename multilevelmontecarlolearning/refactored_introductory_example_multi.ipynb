{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/da-roth/NeuronalNetworkTensorflowFramework/blob/main/multilevelmontecarlolearning/new_introductory_example_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU support =  False\n",
      "150\n",
      "300\n",
      "450\n",
      "600\n",
      "750\n",
      "900\n",
      "1050\n",
      "1200\n",
      "1350\n",
      "1500\n",
      "1650\n",
      "1800\n",
      "1950\n",
      "2100\n",
      "2250\n",
      "2400\n",
      "2550\n",
      "2700\n",
      "2850\n",
      "3000\n",
      "3150\n",
      "3300\n",
      "3450\n",
      "3600\n",
      "3750\n",
      "3900\n",
      "4050\n",
      "4200\n",
      "4350\n",
      "4500\n",
      "4650\n",
      "4800\n",
      "4950\n",
      "5100\n",
      "5250\n",
      "5400\n",
      "5550\n",
      "5700\n",
      "5850\n",
      "6000\n",
      "6150\n",
      "6300\n",
      "6450\n",
      "6600\n",
      "6750\n",
      "6900\n",
      "7050\n",
      "7200\n",
      "7350\n",
      "7500\n",
      "7650\n",
      "7800\n",
      "7950\n",
      "8100\n",
      "8250\n",
      "8400\n",
      "8550\n",
      "8700\n",
      "8850\n",
      "9000\n",
      "9150\n",
      "9300\n",
      "9450\n",
      "9600\n",
      "9750\n",
      "9900\n",
      "10050\n",
      "10200\n",
      "10350\n",
      "10500\n",
      "10650\n",
      "10800\n",
      "10950\n",
      "11100\n",
      "11250\n",
      "11400\n",
      "11550\n",
      "11700\n",
      "11850\n",
      "12000\n",
      "12150\n",
      "12300\n",
      "12450\n",
      "12600\n",
      "12750\n",
      "12900\n",
      "13050\n",
      "13200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\multilevelmontecarlolearning\\refactored_introductory_example_multi.ipynb Cell 2\u001b[0m in \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=334'>335</a>\u001b[0m     u_reference_list\u001b[39m.\u001b[39mappend(xi_approx\u001b[39m*\u001b[39m\u001b[39m0.\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=336'>337</a>\u001b[0m \u001b[39m#Start training and testing                        \u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=337'>338</a>\u001b[0m train_and_test(Regressor, TrainSettings,xi_list, phi_list, xi_approx, u_reference, u_reference_list, neurons, \u001b[39m'\u001b[39;49m\u001b[39mmulti-introductory-new-2.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, dtype)                \n",
      "\u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\multilevelmontecarlolearning\\refactored_introductory_example_multi.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m         \u001b[39mprint\u001b[39m(step)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m         t1_train \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m         _approximate_errors()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m         t0_train \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()      \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m sess\u001b[39m.\u001b[39mrun(train_op, feed_dict\u001b[39m=\u001b[39m{is_training:\u001b[39mTrue\u001b[39;00m})\n",
      "\u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\multilevelmontecarlolearning\\refactored_introductory_example_multi.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m li_err_kombination \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(TrainSettings\u001b[39m.\u001b[39mmcRounds):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     li \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39;49mrun([err_l_inf[i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(amountNetworks)], feed_dict\u001b[39m=\u001b[39;49m{is_training: \u001b[39mFalse\u001b[39;49;00m})\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     appr_ref_kombination \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39mrun([approx[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(amountNetworks)] \u001b[39m+\u001b[39m [reference, err_l_kombination], feed_dict\u001b[39m=\u001b[39m{is_training: \u001b[39mFalse\u001b[39;00m})\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/DanielPrivateGitHub/Documents/GitHub/NeuronalNetworkTensorflowFramework/multilevelmontecarlolearning/refactored_introductory_example_multi.ipynb#W1sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m     appr \u001b[39m=\u001b[39m appr_ref_kombination[:amountNetworks]\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[0;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m   1372\u001b[0m                        run_metadata)\n\u001b[0;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[0;32m   1377\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   1379\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1380\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1359\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[1;32mc:\\Users\\DanielPrivateGitHub\\Documents\\GitHub\\NeuronalNetworkTensorflowFramework\\.venvNN\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[0;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[0;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "#Multilevel algorithm using 8 networks\n",
    "#For more detailed explanations of the training and model parameters\n",
    "#see Gerstner et al. \"Multilevel Monte Carlo learning.\" arXiv preprint arXiv:2102.08734 (2021).\n",
    "\n",
    "#Basic network framework according to Beck, Christian, et al. \"Solving the Kolmogorov PDE by means of deep learning.\" Journal of Scientific Computing 88.3 (2021): 1-28.\n",
    "#The framework was modified in such a way that it generates networks for each of the level estimators\n",
    "\n",
    "import os\n",
    "mainDirectory = os.path.abspath(os.path.join(os.getcwd() ,'..'))\n",
    "packageFile = os.path.abspath(os.path.join(mainDirectory, 'montecarlolearning', 'packages.py'))\n",
    "exec(open(packageFile).read())\n",
    "\n",
    "#Packages\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "\n",
    "class GBM_Multilevel:\n",
    "    @staticmethod\n",
    "    def phi(x,sigma,mu,T,K, axis=1):\n",
    "        payoff=tf.exp(-mu * T)* tf.maximum(x - K, 0.)\n",
    "        return payoff\n",
    "    \n",
    "    @staticmethod\n",
    "    # First level: actually just P_0 without level estimator\n",
    "    def Milstein_level0(idx, s,sigma,mu,T,K, samples): \n",
    "        z = tf.random_normal(shape=(samples, batch_sizes[0], 1),\n",
    "                            stddev=1., dtype=dtype)\n",
    "        h=T/stepsPerLevel[0]\n",
    "        s=s + mu *s * h +sigma * s *tf.sqrt(h)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(h)*z)**2-h)\n",
    "        return tf.add(idx, 1), s, sigma,mu,T,K\n",
    "    \n",
    "    @staticmethod\n",
    "    def MonteCarlo_loop_level0(idx, p):\n",
    "        _, _x, _sigma,_mu,_T,_K = tf.while_loop(lambda _idx, s, sigma,mu,T,K: _idx < stepsPerLevel[0],\n",
    "                            lambda _idx, s, sigma,mu,T,K: GBM_Multilevel.Milstein_level0(_idx, s, sigma,mu,T,K,\n",
    "                                                    mc_samples_ref),\n",
    "                                                    loop_var_mc[0])\n",
    "        return idx + 1, p + tf.reduce_mean(GBM_Multilevel.phi(_x,_sigma,_mu,_T,_K, 2), axis=0)  \n",
    "        \n",
    "    #Multilevel Monte Carlo level estimators\n",
    "    @staticmethod\n",
    "    def Milstein_levelEstimator(idx, s, sfine, sigma, mu, T, K, samples, level): \n",
    "        z1 = tf.random_normal(shape=(samples, batch_sizes[level], 1),\n",
    "                            stddev=1., dtype=dtype)\n",
    "        z2 = tf.random_normal(shape=(samples, batch_sizes[level], 1),\n",
    "                            stddev=1., dtype=dtype)\n",
    "        z=(z1+z2)/tf.sqrt(2.)\n",
    "        amountSteps = stepsPerLevel[level-1]\n",
    "        hcoarse= T / amountSteps\n",
    "        hfine= T / (amountSteps * 2)\n",
    "        sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z1 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z1)**2-hfine)\n",
    "        sfine=sfine + mu *sfine * hfine +sigma * sfine *tf.sqrt(hfine)*z2 + 0.5 *sigma *sfine *sigma * ((tf.sqrt(hfine)*z2)**2-hfine)\n",
    "        s=s + mu *s * hcoarse +sigma * s *tf.sqrt(hcoarse)*z + 0.5 *sigma *s *sigma * ((tf.sqrt(hcoarse)*z)**2-hcoarse)    \n",
    "        return tf.add(idx, 1), s, sfine, sigma, mu, T, K\n",
    "\n",
    "    @staticmethod\n",
    "    def MonteCarlo_loop_levelEstimator(idx, p, level):\n",
    "        amountSteps = stepsPerLevel[level-1]\n",
    "        _, _xcoarse, _xfine, sigma, mu, T, K = tf.while_loop(lambda _idx, s, xfine, sigma, mu, T, K: _idx < amountSteps,\n",
    "                                                            lambda _idx, s, xfine, sigma, mu, T, K: GBM_Multilevel.Milstein_levelEstimator(_idx, s, xfine, sigma, mu, T, K, mc_samples_ref, level),\n",
    "                                                            loop_var_mc[level])\n",
    "        return idx + 1, p + tf.reduce_mean(GBM_Multilevel.phi(_xfine, sigma, mu, T, K, 2) - GBM_Multilevel.phi(_xcoarse, sigma, mu, T, K, 2), axis=0)\n",
    "\n",
    "\n",
    "class Neural_Approximator_Multilevel:\n",
    "        # Setter for data Generator\n",
    "    def set_Generator(self, Generator):\n",
    "        self._Generator = Generator\n",
    "        \n",
    "    @property\n",
    "    def Generator(self):\n",
    "        return self._Generator\n",
    "\n",
    "    # Setter for hiddenNeurons\n",
    "    def set_hiddenNeurons(self, hiddenNeurons):\n",
    "        self._hiddenNeurons = hiddenNeurons\n",
    "\n",
    "    # Setter for hiddenLayers\n",
    "    def set_hiddenLayers(self, hiddenLayers):\n",
    "        self._hiddenLayers = hiddenLayers\n",
    "        \n",
    "    @property\n",
    "    def HiddenNeurons(self):\n",
    "        return self._hiddenNeurons\n",
    "    \n",
    "    @property\n",
    "    def HiddenLayers(self):\n",
    "        return self._hiddenLayers\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def neural_net(x, xi_approx, neurons, is_training, name, net_id, mv_decay=0.9, dtype=tf.float32):\n",
    "        def approx_test(): return xi_approx\n",
    "        def approx_learn(): return x\n",
    "        x = tf.cond(is_training, approx_learn, approx_test)\n",
    "\n",
    "        def _batch_normalization(_x):\n",
    "            beta = tf.get_variable(f'beta{net_id}', [_x.get_shape()[-1]], dtype, init_ops.zeros_initializer())\n",
    "            gamma = tf.get_variable(f'gamma{net_id}', [_x.get_shape()[-1]], dtype, init_ops.ones_initializer())\n",
    "            mv_mean = tf.get_variable(f'mv_mean{net_id}', [_x.get_shape()[-1]], dtype, init_ops.zeros_initializer(), trainable=False)\n",
    "            mv_variance = tf.get_variable(f'mv_variance{net_id}', [_x.get_shape()[-1]], dtype, init_ops.ones_initializer(), trainable=False)\n",
    "            mean, variance = tf.nn.moments(_x, [0], name=f'moments{net_id}')\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, assign_moving_average(mv_mean, mean, mv_decay, True))\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, assign_moving_average(mv_variance, variance, mv_decay, False))\n",
    "            mean, variance = tf.cond(is_training, lambda: (mean, variance), lambda: (mv_mean, mv_variance))\n",
    "            return tf.nn.batch_normalization(_x, mean, variance, beta, gamma, 1e-6)\n",
    "\n",
    "        def _layer(_x, out_size, activation_fn):\n",
    "            w = tf.get_variable(f'weights{net_id}', [_x.get_shape().as_list()[-1], out_size], dtype, tf.initializers.glorot_uniform())\n",
    "            return activation_fn(_batch_normalization(tf.matmul(_x, w)))\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            x = _batch_normalization(x)\n",
    "            for i in range(len(neurons)):\n",
    "                with tf.variable_scope(f'layer{net_id}_{i + 1}_'):\n",
    "                    x = _layer(x, neurons[i], tf.nn.tanh if i < len(neurons)-1 else tf.identity)\n",
    "        return x\n",
    "\n",
    "def train_and_test(Regressor, TrainSettings, xi_list, phi_list, xi_approx, u_reference_GBM, u_reference_list, neurons, file_name, dtype=tf.float32):\n",
    "    \n",
    "    def _approximate_errors():\n",
    "        gs_lr = sess.run([global_step[i] for i in range(amountNetworks)] + [learning_rate[i] for i in range(amountNetworks)])\n",
    "        gs = gs_lr[:amountNetworks]\n",
    "        lr = gs_lr[amountNetworks:]\n",
    "        li_err = [0. for _ in range(amountNetworks)]\n",
    "        li_err_kombination = 0.\n",
    "        for _ in range(TrainSettings.mcRounds):\n",
    "            li = sess.run([err_l_inf[i] for i in range(amountNetworks)], feed_dict={is_training: False})\n",
    "            appr_ref_kombination = sess.run([approx[i] for i in range(amountNetworks)] + [reference, err_l_kombination], feed_dict={is_training: False})\n",
    "            appr = appr_ref_kombination[:amountNetworks]\n",
    "            ref = appr_ref_kombination[amountNetworks]\n",
    "            li_kombination = appr_ref_kombination[-1]\n",
    "            for i in range(amountNetworks):\n",
    "                li_err[i] = np.maximum(li_err[i], li[i])\n",
    "            li_err_kombination = np.maximum(li_err_kombination, li_kombination)\n",
    "        t_mc = time.time()\n",
    "        file_out.write(f'{gs[0]}, {li_err_kombination}, {lr[0]}, {t1_train - t0_train}, {t_mc - t1_train}\\n')\n",
    "        file_out.flush()\n",
    "    \n",
    "    t0_train = time.time()\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    amountNetworks = len(phi_list)\n",
    "    u_approx = []\n",
    "    for i in range(amountNetworks):\n",
    "        u_approx.append(Regressor.neural_net(xi_list[i], xi_approx, neurons, is_training, f'u_approx_{i}', str(i), dtype=dtype))\n",
    "\n",
    "    loss = [tf.reduce_mean(tf.squared_difference(u_approx[i], phi_list[i])) for i in range(amountNetworks)]\n",
    "\n",
    "    approx = [tf.reduce_mean(u_approx[i]) for i in range(amountNetworks)]\n",
    "    reference = tf.reduce_mean(u_reference_GBM)\n",
    "\n",
    "    err = [tf.abs(u_approx[i] - u_reference_list[i]) for i in range(amountNetworks)]\n",
    "\n",
    "    err_kombination = tf.abs(sum(u_approx) - u_reference_GBM)\n",
    "\n",
    "    err_l_inf = [tf.reduce_max(err[i]) for i in range(len(err))]\n",
    "    err_l_kombination = tf.reduce_max(err_kombination)\n",
    "\n",
    "    lr = [TrainSettings.learningRateSchedule[0] for _ in range(amountNetworks)]\n",
    "    step_rate = [TrainSettings.learningRateSchedule[2] for _ in range(amountNetworks)]\n",
    "    decay = [TrainSettings.learningRateSchedule[1] for _ in range(amountNetworks)]\n",
    "    global_step = [tf.Variable(1, trainable=False) for _ in range(amountNetworks)]\n",
    "    increment_global_step = [tf.assign(global_step[i], global_step[i] + 1) for i in range(amountNetworks)]\n",
    "    learning_rate = [tf.train.exponential_decay(lr[i], global_step[i], step_rate[i], decay[i], staircase=True) for i in range(amountNetworks)]\n",
    "    optimizer = [tf.train.AdamOptimizer(learning_rate[i]) for i in range(amountNetworks)]\n",
    "    update_ops = [tf.get_collection(tf.GraphKeys.UPDATE_OPS, f'u_approx_{i}') for i in range(amountNetworks)]\n",
    "    \n",
    "    train_op = []\n",
    "    for i in range(len(update_ops)):\n",
    "        with tf.control_dependencies(update_ops[i]):\n",
    "            train_op.append(optimizer[i].minimize(loss[i], global_step[i])) \n",
    "            \n",
    "    file_out = open(file_name, 'w')\n",
    "    file_out.write('step,li_err, learning_rate, time_train, time_mc  \\n ')\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(1,TrainSettings.TrainingSteps):\n",
    "            if step % TrainSettings.testFrequency == 0:\n",
    "                print(step)\n",
    "                t1_train = time.time()\n",
    "                _approximate_errors()\n",
    "                t0_train = time.time()      \n",
    "        sess.run(train_op, feed_dict={is_training:True})\n",
    "        t1_train = time.time()\n",
    "        _approximate_errors()\n",
    "    file_out.close()\n",
    "\n",
    "Generator = GBM_Multilevel()\n",
    "  \n",
    "Regressor = Neural_Approximator_Multilevel()\n",
    "Regressor.set_hiddenNeurons(50)\n",
    "Regressor.set_hiddenLayers(2)\n",
    "\n",
    "TrainSettings = TrainingSettings()\n",
    "TrainSettings.set_learning_rate_schedule([0.01, 0.1, 1000])\n",
    "TrainSettings.set_test_frequency(150)\n",
    "TrainSettings.set_mcRounds(100)\n",
    "TrainSettings.set_nTest(200)\n",
    "TrainSettings.set_samplesPerStep([75000, 1817, 690, 264, 93, 33, 12, 5])\n",
    "TrainSettings.set_trainingSteps(150000)\n",
    "\n",
    "#Model and training parameter specification  \n",
    "for i in range(1,2):\n",
    "    #print(i)\n",
    "    tf.reset_default_graph()\n",
    "    tf.random.set_random_seed(i)\n",
    "    with tf.Session()  as sess:\n",
    "        dtype = tf.float32\n",
    "        #Set network and training parameter (same number of training steps for each network)\n",
    "        batch_sizes = TrainSettings.SamplesPerStep # original [75000, 1817, 690, 264, 93, 33, 12, 5]\n",
    "        batch_size_approx= TrainSettings.nTest# original 2000000\n",
    "        d = 5\n",
    "        # Level adaptation parameter: steps = M ^ l\n",
    "        M = 2\n",
    "        maximumLevel = len(batch_sizes) - 1 # P_0 + P_1-P_0, here 1 is the maximumLevel\n",
    "        stepsPerLevel = [M**i for i in range(maximumLevel)]\n",
    "\n",
    "        neurons = [Regressor.HiddenNeurons for _ in range(Regressor.HiddenLayers)] + [1]\n",
    "        train_steps = TrainSettings.TrainingSteps # original 150000\n",
    "        \n",
    "        Ksteps_p1_p0=train_steps\n",
    "        Ksteps_p0=   train_steps    \n",
    "        mc_rounds, mc_freq = TrainSettings.mcRounds, TrainSettings.testFrequency # original  100, 10\n",
    "\n",
    "        mc_samples_ref, mc_rounds_ref_p0, mc_rounds_ref_p1_p0 = 1, 1000000,1000000\n",
    "\n",
    "        # Define the intervals for the parameters\n",
    "        s_0_l = 80.0\n",
    "        s_0_r = 120.0\n",
    "        sigma_l = 0.1\n",
    "        sigma_r = 0.2\n",
    "        mu_l = 0.02\n",
    "        mu_r = 0.05\n",
    "        T_l = 0.9\n",
    "        T_r = 1.0\n",
    "        K_l = 109.0\n",
    "        K_r = 110.0\n",
    "\n",
    "        # Define the modifiers for the approximation intervals\n",
    "        s_0_modifier = 0.4\n",
    "        sigma_modifier = 0.01\n",
    "        mu_modifier = 0.01\n",
    "        T_modifier = 0.01\n",
    "        K_modifier = 0.1\n",
    "\n",
    "        # Use the modifiers to define the intervals for the approximations of the parameters\n",
    "        s_0_l_approx = s_0_l + s_0_modifier\n",
    "        s_0_r_approx = s_0_r - s_0_modifier\n",
    "        sigma_l_approx = sigma_l + sigma_modifier\n",
    "        sigma_r_approx = sigma_r - sigma_modifier\n",
    "        mu_l_approx = mu_l + mu_modifier\n",
    "        mu_r_approx = mu_r - mu_modifier\n",
    "        T_l_approx = T_l + T_modifier\n",
    "        T_r_approx = T_r - T_modifier\n",
    "        K_l_approx = K_l + K_modifier\n",
    "        K_r_approx = K_r - K_modifier\n",
    "\n",
    "        # Training intervals\n",
    "        s0 = tf.random_uniform((batch_sizes[0],1), minval=s_0_l,\n",
    "                                        maxval=s_0_r, dtype=dtype)\n",
    "        sigma=tf.random_uniform((batch_sizes[0],1),\n",
    "                                    minval=sigma_l,maxval=sigma_r, dtype=dtype)\n",
    "        mu=tf.random_uniform((batch_sizes[0],1),\n",
    "                                    minval=mu_l,maxval=mu_r, dtype=dtype)\n",
    "        T=tf.random_uniform((batch_sizes[0],1),\n",
    "                                    minval=T_l,maxval=T_r, dtype=dtype)\n",
    "        K=tf.random_uniform((batch_sizes[0],1),\n",
    "                            minval=K_l,maxval=K_r, dtype=dtype)\n",
    "        \n",
    "        xi_level0=tf.reshape(tf.stack([s0,sigma,mu,T,K], axis=2), (batch_sizes[0],d))\n",
    "\n",
    "        xi_list = []\n",
    "        xi_list.append(xi_level0)\n",
    "\n",
    "        loop_var_mc = []\n",
    "        loop_var_mc.append((tf.constant(0),tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * s0, tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * sigma,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * mu,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * T,tf.ones((mc_samples_ref,batch_sizes[0], 1), dtype) * K))\n",
    "        \n",
    "\n",
    "        for i in range(1,len(batch_sizes)):\n",
    "            s0_level_estimator = tf.stack((tf.random_uniform((batch_sizes[i],1), minval=s_0_l, maxval=s_0_r, dtype=dtype)))\n",
    "            sigma_level_estimator = tf.random_uniform((batch_sizes[i],1), minval=sigma_l, maxval=sigma_r, dtype=dtype)\n",
    "            mu_level_estimator = tf.random_uniform((batch_sizes[i],1), minval=mu_l, maxval=mu_r, dtype=dtype)\n",
    "            T_level_estimator = tf.random_uniform((batch_sizes[i],1), minval=T_l, maxval=T_r, dtype=dtype)\n",
    "            K_level_estimator = tf.random_uniform((batch_sizes[i],1), minval=K_l, maxval=K_r, dtype=dtype)\n",
    "            xi_level_estimator= tf.reshape(tf.stack([s0_level_estimator, sigma_level_estimator, mu_level_estimator, T_level_estimator, K_level_estimator], axis=2), (batch_sizes[i], d))\n",
    "            xi_list.append(xi_level_estimator)\n",
    "            loop_var_mc.append((tf.constant(0),tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * s0_level_estimator,tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * s0_level_estimator, tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * sigma_level_estimator,tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * mu_level_estimator,tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * T_level_estimator,tf.ones((mc_samples_ref,batch_sizes[i], 1), dtype) * K_level_estimator))\n",
    "\n",
    "        # Approximation intervals\n",
    "        s0_approx = tf.random_uniform((batch_size_approx,  1 ), \n",
    "                                    minval=s_0_l_approx,maxval=s_0_r_approx, dtype=dtype)\n",
    "        sigma_approx=tf.random_uniform((batch_size_approx,  1 ), \n",
    "                                    minval=sigma_l_approx,maxval=sigma_r_approx, dtype=dtype)\n",
    "        mu_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                                    minval=mu_l_approx,maxval=mu_r_approx, dtype=dtype)\n",
    "        T_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                                    minval=T_l_approx,maxval=T_r_approx, dtype=dtype)\n",
    "        K_approx=tf.random_uniform((batch_size_approx,1),\n",
    "                                    minval=K_l_approx,maxval=K_r_approx, dtype=dtype)\n",
    "        xi_approx=tf.reshape(tf.stack([s0_approx,sigma_approx,mu_approx,T_approx,K_approx], axis=2), (batch_size_approx,d))\n",
    "\n",
    "        # References: Black-Scholes formula as reference\n",
    "        tfd = tfp.distributions\n",
    "        dist = tfd.Normal(loc=tf.cast(0.,tf.float32), scale=tf.cast(1.,tf.float32))\n",
    "        d1=tf.math.divide(\n",
    "        (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx + 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
    "        d2=tf.math.divide(\n",
    "        (tf.log(tf.math.divide(s0_approx,K_approx))+(mu_approx - 0.5*sigma_approx**2)*T_approx) , (sigma_approx*tf.sqrt(T_approx)))\n",
    "\n",
    "        u_reference= tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2))\n",
    "\n",
    "    u_list = []\n",
    "    phi_list = []\n",
    "    u_reference_list =[]\n",
    "\n",
    "    u_list.append(tf.while_loop(lambda idx, p: idx < 1, Generator.MonteCarlo_loop_level0,(tf.constant(0), tf.zeros((batch_sizes[0], 1), dtype)))[1])\n",
    "    phi_list.append(u_list[0] / tf.cast(1, tf.float32))\n",
    "    u_reference_list.append(tf.multiply(s0_approx,(dist.cdf(d1)))-K_approx*tf.exp(-mu_approx*T_approx)*(dist.cdf(d2)))\n",
    "\n",
    "\n",
    "    for i in range(1,len(batch_sizes)):\n",
    "        u_list.append(tf.while_loop(lambda idx, p: idx < 1, lambda idx, p: Generator.MonteCarlo_loop_levelEstimator(idx, p, i), (tf.constant(0), tf.zeros((batch_sizes[i], 1), dtype)))[1])\n",
    "        phi_list.append(u_list[i] / tf.cast(1, tf.float32))\n",
    "        u_reference_list.append(xi_approx*0.)\n",
    "\n",
    "    #Start training and testing                        \n",
    "    train_and_test(Regressor, TrainSettings,xi_list, phi_list, xi_approx, u_reference, u_reference_list, neurons, 'multi-introductory-new-2.csv', dtype)                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
